{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from u_cmab import Train, qini\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"http://www.minethatdata.com/Kevin_Hillstrom_MineThatData_E-MailAnalytics_DataMiningChallenge_2008.03.20.csv\")\n",
    "data[\"segment\"] = data[\"segment\"].astype(\"category\")\n",
    "data[\"history_segment\"] = data[\"history_segment\"].astype(\"category\")\n",
    "data[\"zip_code\"] = data[\"zip_code\"].astype(\"category\")\n",
    "data[\"channel\"] = data[\"channel\"].astype(\"category\")\n",
    "\n",
    "data_size = data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_hillstrom(data):\n",
    "    context = data[[\"recency\", \"history_segment\", \"history\", \"mens\", \"womens\", \"zip_code\", \"newbie\", \"channel\"]].copy()\n",
    "    treatment = data[\"segment\"]\n",
    "    reward = data[\"visit\"]\n",
    "\n",
    "    one_hot_hs = pd.get_dummies(context[\"history_segment\"], prefix=\"hs\")\n",
    "    one_hot_zc = pd.get_dummies(context[\"zip_code\"], prefix=\"zc\")\n",
    "    one_hot_c = pd.get_dummies(context[\"channel\"], prefix=\"c\")\n",
    "\n",
    "    context = pd.concat([context[[\"recency\", \"history\", \"mens\", \"womens\", \"newbie\"]], one_hot_hs, one_hot_zc, one_hot_c], axis=1)\n",
    "\n",
    "    return (context.values, treatment.values, reward.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "experiment runs `10` times: trains ANN for 400 epochs and 2 URFs each time, this takes considerable time on a PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 35\n",
    "context_n = 18\n",
    "treatment_n = 3\n",
    "\n",
    "forests_m = []\n",
    "forests_w = []\n",
    "nets_m = []\n",
    "nets_w = []\n",
    "treats_m = []\n",
    "treats_w = []\n",
    "resps_m = []\n",
    "resps_w = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: PYDEVD_USE_CYTHON environment variable is set to 'NO'. Frame evaluator will be also disabled because it requires Cython extensions to be enabled in order to operate correctly.\n",
      "/home/agni-ubuntu/study_mat/adv_stats/proj/u-cmab/u_cmab/data_test.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['strat'] = self.data[\"segment\"].astype(str) + self.data[\"visit\"].astype(str)\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=10)]: Done 250 out of 250 | elapsed: 14.2min finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:   57.0s\n",
      "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=10)]: Done 250 out of 250 | elapsed: 14.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    }
   ],
   "source": [
    "runs = 10\n",
    "for i in range(runs):\n",
    "    t = Train(data, parse_data_hillstrom)\n",
    "    \n",
    "    # 2 RF's\n",
    "    u_w = t.rf(segment=\"w\")\n",
    "    u_m = t.rf(segment=\"m\")\n",
    "    \n",
    "    # 1 NET\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(context_n, 36),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(36, 36),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(36, 36),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(36, 18),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(18, treatment_n)\n",
    "    )\n",
    "    t.nn(net, epochs=400, weight_decay=0, loss_f=nn.SmoothL1Loss(), batch_size=32, learning_rate=1e-5)\n",
    "    \n",
    "    # Calculate results of single net for 2 groups (w and m)\n",
    "    c_w, tr_w, r_w = parse_data_hillstrom(t.test_data_w)\n",
    "    c_m, tr_m, r_m = parse_data_hillstrom(t.test_data_m)\n",
    "\n",
    "    tr_w = tr_w.codes - 1\n",
    "    tr_m = abs(tr_m.codes - 1)\n",
    "        \n",
    "    res_w = net(torch.tensor(c_w, dtype=torch.float, requires_grad=False))\n",
    "    u_hat_w = (res_w[:,2] - res_w[:,1]).detach().numpy()\n",
    "    res_m = net(torch.tensor(c_m, dtype=torch.float, requires_grad=False))\n",
    "    u_hat_m = (res_m[:,0] - res_m[:,1]).detach().numpy()\n",
    "    \n",
    "    \n",
    "    # Document results\n",
    "    forests_w.append(u_w)\n",
    "    forests_m.append(u_m)\n",
    "    nets_w.append(u_hat_w)\n",
    "    nets_m.append(u_hat_m)\n",
    "    treats_w.append(tr_w)\n",
    "    treats_m.append(tr_m)\n",
    "    resps_w.append(r_w)\n",
    "    resps_m.append(r_m)\n",
    "    \n",
    "    print(f\"END OF RUN {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax1 = qini([treats_w, treats_m], [resps_w, resps_m], [nets_w, nets_m], [forests_w, forests_m], \n",
    "               urf_label=['URF (T=1)', 'URF (T=2)'], \n",
    "               urf_colors=['firebrick', 'goldenrod'], \n",
    "               urf_colors_bands=['deeppink', 'orange'],\n",
    "               treatment_names=['T=1', \"T=2\"],\n",
    "               yticks=[0, .04, .08])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}